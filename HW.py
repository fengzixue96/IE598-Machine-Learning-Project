import pandas as pd
import numpy as np
import sys
import matplotlib.pyplot as plt
import scipy.stats as stats
import seaborn as sns
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split,cross_val_score,KFold,GridSearchCV
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression,Lasso,Ridge
from sklearn import svm
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor

df = pd.read_csv('C:/Users/HP/Desktop/Lesson/598/Assignment/Group Project/MLF_GP2_EconCycle.csv')
df = df.drop(['Date','USPHCI'],axis=1)
columns=df.columns[:]

#Read and Summarize Data
print(df.shape)
row=df.shape[0]
column=df.shape[1]
print(df.head())
print(df.tail())
summary = df.describe()
print(summary)
print(df.info())

#summary statistics for each feature/target column
def ecdf(data):
    n = len(data)
    x = np.sort(data)
    y = np.arange(1, n+1) / n
    return x, y
for i in [0,5,6,8,9,11,12,13,14]:
    sys.stdout.write("The summary statistics of " + str(columns[i]) + "\n")
    mean = np.mean(df.iloc[:,i])
    std = np.std(df.iloc[:,i])
    sys.stdout.write("Mean = " + '\t' + str(mean) + '\t\t' + "Standard Deviation = " + '\t ' + str(std) + "\n")
    percentiles = np.array([2.5, 25, 50, 75, 97.5])
    ptiles_vers = np.percentile(df.iloc[:,i], percentiles)
    sys.stdout.write("\nBoundaries for 4 Equal Percentiles \n")
    print(ptiles_vers)
    sys.stdout.write(" \n")
    plt.figure()
    x_vers, y_vers=ecdf(df.iloc[:,i])
    plt.plot(x_vers, y_vers, '.')
    plt.xlabel(columns[i])
    plt.ylabel('ECDF')
    plt.title("Percentiles ECDF of column "+str(columns[i]))
    plt.plot(ptiles_vers, percentiles/100, marker='D', color='red', linestyle='none')
    plt.show()

#Quantile‚ÄêQuantile Plot for each feature/target column
for i in [0,5,6,8,9,11,12,13,14]:
    plt.figure()
    stats.probplot(df.iloc[:,i], dist="norm",plot=plt)
    plt.show()

#graphical summary of the relationships
df1=df.drop(['T2Y Index','T3Y Index','T5Y Index','T7Y Index','CP3M','CP3M_T1Y'],axis=1)
col=df1.columns[:]

#Corr
plt.figure()
sns.pairplot(df[col], size=2.5)
plt.tight_layout()
plt.show()

#Heatmap
cm = np.corrcoef(df[col].values.T)
sns.set(font_scale=1.5)
hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 15}, yticklabels=col, xticklabels=col)
plt.show()

#Preprocessing
#Since there is no NaN in the data, so it is unnecessary to eliminate missing data

#X--Sdandarize & PCA
X=df.drop(['PCT 3MO FWD','PCT 6MO FWD','PCT 9MO FWD'],axis=1).values
steps=[('scaler',StandardScaler()),('pca',PCA())]
pipeline=Pipeline(steps)
pipeline.fit(X)
X_=pipeline.transform(X)
cov_mat = np.cov(X_.T)
eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)
tot = sum(eigen_vals)
var_exp = [(i/tot) for i in sorted(eigen_vals,reverse=True)]
print('Explained variance ratio: ', pipeline['pca'].explained_variance_ratio_)
print('Explained variance: ', pipeline['pca'].explained_variance_)
cum_var_exp = np.cumsum(var_exp)
plt.figure()
plt.bar(range(1,13),var_exp,alpha=0.5,align='center',label='individual explained variance')
plt.step(range(1,13),cum_var_exp,where='mid',label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal component index')
plt.legend(loc='best')
plt.show()

#from PCA's figure, we know that we should only choose first three PCA columns, n_component=3
pca = PCA(n_components=2)
pca.fit(X)
X_pca = pca.transform(X)

#y--Sdandarize
y1=preprocessing.scale(df['PCT 3MO FWD'].values)
y2=preprocessing.scale(df['PCT 6MO FWD'].values)
y3=preprocessing.scale(df['PCT 9MO FWD'].values)

#split data for 3 groups
X1_train,X1_test,y1_train,y1_test=train_test_split(X_pca,y1,test_size=0.3,random_state=42)
X2_train,X2_test,y2_train,y2_test=train_test_split(X_pca,y2,test_size=0.3,random_state=42)
X3_train,X3_test,y3_train,y3_test=train_test_split(X_pca,y3,test_size=0.3,random_state=42)

#Group1: Linear Regression with Kfold CV
print('Group 1 Linear')
LR=LinearRegression()
LR.fit(X1_train, y1_train)
cv = KFold(5, shuffle=True, random_state=33)
scores_train1 = cross_val_score(LR, X1_train, y1_train, cv=cv)
print("Average accuracy score on training set using 5-fold cross-validation:",np.mean(scores_train1))
scores_test1 = cross_val_score(LR, X1_test, y1_test, cv=cv)
print("Average accuracy score on testing set using 5-fold cross-validation:",np.mean(scores_test1))
y1_train_pred = LR.predict(X1_train)
print("Predictions of train set:")
print(y1_train_pred)
y1_test_pred = LR.predict(X1_test)
print("Predictions of test set:")
print(y1_test_pred)
rmse = np.sqrt(mean_squared_error(y1_train,y1_train_pred))
print("Root Mean Squared Error of Train Set: {}".format(rmse))
rmse = np.sqrt(mean_squared_error(y1_test,y1_test_pred))
print("Root Mean Squared Error of Test Set: {}".format(rmse))
print('Slope:')
print(LR.coef_)
print('Intercept: %.3f' % LR.intercept_)
plt.figure()
plt.scatter(y1_train_pred, y1_train_pred - y1_train, c='steelblue', marker='o', edgecolor='white', label='Training data')
plt.scatter(y1_test_pred, y1_test_pred - y1_test, c='limegreen', marker='s', edgecolor='white', label='Test data')
plt.xlabel('Predicted values')
plt.ylabel('Residuals')
plt.title('Linear Regression')
plt.legend(loc='upper left')
plt.hlines(y=0, xmin=-10, xmax=50, color='black', lw=2)
plt.xlim([-10, 50])
plt.show()

#Group1: Lasso Regression with Kfold CV
print('Group 1 Lasso')
lasso = Lasso(alpha=1.0)
lasso.fit(X1_train, y1_train)
print(lasso.score(X1_test, y1_test))
cv = KFold(5, shuffle=True, random_state=33)
scores_train1 = cross_val_score(lasso, X1_train, y1_train, cv=cv)
print("Average accuracy score on training set using 5-fold cross-validation:",np.mean(scores_train1))
scores_test1 = cross_val_score(lasso, X1_test, y1_test, cv=cv)
print("Average accuracy score on testing set using 5-fold cross-validation:",np.mean(scores_test1))
y1_train_pred = lasso.predict(X1_train)
print("Predictions of train set:")
print(y1_train_pred)
y1_test_pred = lasso.predict(X1_test)
print("Predictions of test set:")
print(y1_test_pred)
rmse = np.sqrt(mean_squared_error(y1_train,y1_train_pred))
print("Root Mean Squared Error of Train Set: {}".format(rmse))
rmse = np.sqrt(mean_squared_error(y1_test,y1_test_pred))
print("Root Mean Squared Error of Test Set: {}".format(rmse))
print('Slope:')
print(lasso.coef_)
print('Intercept: %.3f' % lasso.intercept_)
plt.figure()
plt.scatter(y1_train_pred, y1_train_pred - y1_train, c='steelblue', marker='o', edgecolor='white', label='Training data')
plt.scatter(y1_test_pred, y1_test_pred - y1_test, c='limegreen', marker='s', edgecolor='white', label='Test data')
plt.xlabel('Predicted values')
plt.ylabel('Residuals')
plt.title('Lasso Regression')
plt.legend(loc='upper left')
plt.hlines(y=0, xmin=-10, xmax=50, color='black', lw=2)
plt.xlim([-10, 50])
plt.show()

#Group1: Ridge Regression with Kfold CV
print('Group 1 Ridge')
ridge = Ridge(alpha=1.0)
ridge.fit(X1_train, y1_train)
cv = KFold(5, shuffle=True, random_state=33)
scores_train1 = cross_val_score(ridge, X1_train, y1_train, cv=cv)
print("Average accuracy score on training set using 5-fold cross-validation:",np.mean(scores_train1))
scores_test1 = cross_val_score(ridge, X1_test, y1_test, cv=cv)
print("Average accuracy score on testing set using 5-fold cross-validation:",np.mean(scores_test1))
y1_train_pred = ridge.predict(X1_train)
print("Predictions of train set:")
print(y1_train_pred)
y1_test_pred = ridge.predict(X1_test)
print("Predictions of test set:")
print(y1_test_pred)
rmse = np.sqrt(mean_squared_error(y1_train,y1_train_pred))
print("Root Mean Squared Error of Train Set: {}".format(rmse))
rmse = np.sqrt(mean_squared_error(y1_test,y1_test_pred))
print("Root Mean Squared Error of Test Set: {}".format(rmse))
print('Slope:')
print(ridge.coef_)
print('Intercept: %.3f' % ridge.intercept_)
plt.figure()
plt.scatter(y1_train_pred, y1_train_pred - y1_train, c='steelblue', marker='o', edgecolor='white', label='Training data')
plt.scatter(y1_test_pred, y1_test_pred - y1_test, c='limegreen', marker='s', edgecolor='white', label='Test data')
plt.xlabel('Predicted values')
plt.ylabel('Residuals')
plt.title('Ridge Regression')
plt.legend(loc='upper left')
plt.hlines(y=0, xmin=-10, xmax=50, color='black', lw=2)
plt.xlim([-10, 50])
plt.show()

#Group1: SVR with Kfold CV
print('Group 1 SVR')
svr = svm.SVR(kernel='linear')
svr.fit(X1_train, y1_train)
cv = KFold(5, shuffle=True, random_state=33)
scores_train1 = cross_val_score(svr, X1_train, y1_train, cv=cv)
print("Average accuracy score on training set using 5-fold cross-validation:",np.mean(scores_train1))
scores_test1 = cross_val_score(svr, X1_test, y1_test, cv=cv)
print("Average accuracy score on testing set using 5-fold cross-validation:",np.mean(scores_test1))
y1_train_pred = svr.predict(X1_train)
print("Predictions of train set:")
print(y1_train_pred)
y1_test_pred = svr.predict(X1_test)
print("Predictions of test set:")
print(y1_test_pred)
rmse = np.sqrt(mean_squared_error(y1_train,y1_train_pred))
print("Root Mean Squared Error of Train Set: {}".format(rmse))
rmse = np.sqrt(mean_squared_error(y1_test,y1_test_pred))
print("Root Mean Squared Error of Test Set: {}".format(rmse))
print('Slope:')
print(svr.coef_)
print('Intercept: %.3f' % svr.intercept_)
plt.figure()
plt.scatter(y1_train_pred, y1_train_pred - y1_train, c='steelblue', marker='o', edgecolor='white', label='Training data')
plt.scatter(y1_test_pred, y1_test_pred - y1_test, c='limegreen', marker='s', edgecolor='white', label='Test data')
plt.xlabel('Predicted values')
plt.ylabel('Residuals')
plt.title('SVR Regression')
plt.legend(loc='upper left')
plt.hlines(y=0, xmin=-10, xmax=50, color='black', lw=2)
plt.xlim([-10, 50])
plt.show()

#Group2: Lasso Regression with Kfold CV
print('Group 2 Lasso')
lasso = Lasso(alpha=1.0)
lasso.fit(X2_train, y2_train)
cv = KFold(5, shuffle=True, random_state=33)
scores_train2 = cross_val_score(lasso, X2_train, y2_train, cv=cv)
print("Average accuracy score on training set using 5-fold cross-validation:",np.mean(scores_train2))
scores_test2 = cross_val_score(lasso, X2_test, y2_test, cv=cv)
print("Average accuracy score on testing set using 5-fold cross-validation:",np.mean(scores_test2))
y2_train_pred = lasso.predict(X2_train)
print("Predictions of train set:")
print(y2_train_pred)
y2_test_pred = lasso.predict(X2_test)
print("Predictions of test set:")
print(y2_test_pred)
rmse = np.sqrt(mean_squared_error(y2_train,y2_train_pred))
print("Root Mean Squared Error of Train Set: {}".format(rmse))
rmse = np.sqrt(mean_squared_error(y2_test,y2_test_pred))
print("Root Mean Squared Error of Test Set: {}".format(rmse))
print('Slope:')
print(lasso.coef_)
print('Intercept: %.3f' % lasso.intercept_)
plt.figure()
plt.scatter(y2_train_pred, y2_train_pred - y2_train, c='steelblue', marker='o', edgecolor='white', label='Training data')
plt.scatter(y2_test_pred, y2_test_pred - y2_test, c='limegreen', marker='s', edgecolor='white', label='Test data')
plt.xlabel('Predicted values')
plt.ylabel('Residuals')
plt.title('Lasso Regression')
plt.legend(loc='upper left')
plt.hlines(y=0, xmin=-10, xmax=50, color='black', lw=2)
plt.xlim([-10, 50])
plt.show()

#Group2: Ridge Regression with Kfold CV
print('Group 2 Ridge')
ridge = Ridge(alpha=1.0)
ridge.fit(X2_train, y2_train)
cv = KFold(5, shuffle=True, random_state=33)
scores_train2 = cross_val_score(ridge, X2_train, y2_train, cv=cv)
print("Average accuracy score on training set using 5-fold cross-validation:",np.mean(scores_train2))
scores_test2 = cross_val_score(ridge, X2_test, y2_test, cv=cv)
print("Average accuracy score on testing set using 5-fold cross-validation:",np.mean(scores_test2))
y2_train_pred = ridge.predict(X2_train)
print("Predictions of train set:")
print(y2_train_pred)
y2_test_pred = ridge.predict(X2_test)
print("Predictions of test set:")
print(y2_test_pred)
rmse = np.sqrt(mean_squared_error(y2_train,y2_train_pred))
print("Root Mean Squared Error of Train Set: {}".format(rmse))
rmse = np.sqrt(mean_squared_error(y2_test,y2_test_pred))
print("Root Mean Squared Error of Test Set: {}".format(rmse))
print('Slope:')
print(ridge.coef_)
print('Intercept: %.3f' % ridge.intercept_)
plt.figure()
plt.scatter(y2_train_pred, y2_train_pred - y2_train, c='steelblue', marker='o', edgecolor='white', label='Training data')
plt.scatter(y2_test_pred, y2_test_pred - y2_test, c='limegreen', marker='s', edgecolor='white', label='Test data')
plt.xlabel('Predicted values')
plt.ylabel('Residuals')
plt.title('Ridge Regression')
plt.legend(loc='upper left')
plt.hlines(y=0, xmin=-10, xmax=50, color='black', lw=2)
plt.xlim([-10, 50])
plt.show()

#Group2: SVR with Kfold CV
print('Group 2 SVR')
svr = svm.SVR(kernel='linear')
svr.fit(X2_train, y2_train)
cv = KFold(5, shuffle=True, random_state=33)
scores_train2 = cross_val_score(svr, X2_train, y2_train, cv=cv)
print("Average accuracy score on training set using 5-fold cross-validation:",np.mean(scores_train2))
scores_test2 = cross_val_score(svr, X2_test, y2_test, cv=cv)
print("Average accuracy score on testing set using 5-fold cross-validation:",np.mean(scores_test2))
y2_train_pred = svr.predict(X2_train)
print("Predictions of train set:")
print(y2_train_pred)
y2_test_pred = svr.predict(X2_test)
print("Predictions of test set:")
print(y2_test_pred)
rmse = np.sqrt(mean_squared_error(y2_train,y2_train_pred))
print("Root Mean Squared Error of Train Set: {}".format(rmse))
rmse = np.sqrt(mean_squared_error(y2_test,y2_test_pred))
print("Root Mean Squared Error of Test Set: {}".format(rmse))
print('Slope:')
print(svr.coef_)
print('Intercept: %.3f' % svr.intercept_)
plt.figure()
plt.scatter(y2_train_pred, y2_train_pred - y2_train, c='steelblue', marker='o', edgecolor='white', label='Training data')
plt.scatter(y2_test_pred, y2_test_pred - y2_test, c='limegreen', marker='s', edgecolor='white', label='Test data')
plt.xlabel('Predicted values')
plt.ylabel('Residuals')
plt.title('SVR Regression')
plt.legend(loc='upper left')
plt.hlines(y=0, xmin=-10, xmax=50, color='black', lw=2)
plt.xlim([-10, 50])
plt.show()

#Group3: Lasso Regression with Kfold CV
print('Group 3 Lasso')
lasso = Lasso(alpha=1.0)
lasso.fit(X3_train, y3_train)
cv = KFold(5, shuffle=True, random_state=33)
scores_train3 = cross_val_score(lasso, X3_train, y3_train, cv=cv)
print("Average accuracy score on training set using 5-fold cross-validation:",np.mean(scores_train3))
scores_test3 = cross_val_score(lasso, X3_test, y3_test, cv=cv)
print("Average accuracy score on testing set using 5-fold cross-validation:",np.mean(scores_test3))
y3_train_pred = lasso.predict(X3_train)
print("Predictions of train set:")
print(y3_train_pred)
y3_test_pred = lasso.predict(X3_test)
print("Predictions of test set:")
print(y3_test_pred)
rmse = np.sqrt(mean_squared_error(y3_train,y3_train_pred))
print("Root Mean Squared Error of Train Set: {}".format(rmse))
rmse = np.sqrt(mean_squared_error(y3_test,y3_test_pred))
print("Root Mean Squared Error of Test Set: {}".format(rmse))
print('Slope:')
print(lasso.coef_)
print('Intercept: %.3f' % lasso.intercept_)
plt.figure()
plt.scatter(y3_train_pred, y3_train_pred - y3_train, c='steelblue', marker='o', edgecolor='white', label='Training data')
plt.scatter(y3_test_pred, y3_test_pred - y3_test, c='limegreen', marker='s', edgecolor='white', label='Test data')
plt.xlabel('Predicted values')
plt.ylabel('Residuals')
plt.title('Lasso Regression')
plt.legend(loc='upper left')
plt.hlines(y=0, xmin=-10, xmax=50, color='black', lw=2)
plt.xlim([-10, 50])
plt.show()

#Group3: Ridge Regression with Kfold CV
print('Group 3 Ridge')
ridge = Ridge(alpha=1.0)
ridge.fit(X3_train, y3_train)
cv = KFold(5, shuffle=True, random_state=33)
scores_train3 = cross_val_score(ridge, X3_train, y3_train, cv=cv)
print("Average accuracy score on training set using 5-fold cross-validation:",np.mean(scores_train3))
scores_test3 = cross_val_score(ridge, X3_test, y3_test, cv=cv)
print("Average accuracy score on testing set using 5-fold cross-validation:",np.mean(scores_test3))
y3_train_pred = ridge.predict(X3_train)
print("Predictions of train set:")
print(y3_train_pred)
y3_test_pred = ridge.predict(X3_test)
print("Predictions of test set:")
print(y3_test_pred)
rmse = np.sqrt(mean_squared_error(y3_train,y3_train_pred))
print("Root Mean Squared Error of Train Set: {}".format(rmse))
rmse = np.sqrt(mean_squared_error(y3_test,y3_test_pred))
print("Root Mean Squared Error of Test Set: {}".format(rmse))
print('Slope:')
print(ridge.coef_)
print('Intercept: %.3f' % ridge.intercept_)
plt.figure()
plt.scatter(y3_train_pred, y3_train_pred - y3_train, c='steelblue', marker='o', edgecolor='white', label='Training data')
plt.scatter(y3_test_pred, y3_test_pred - y3_test, c='limegreen', marker='s', edgecolor='white', label='Test data')
plt.xlabel('Predicted values')
plt.ylabel('Residuals')
plt.title('Ridge Regression')
plt.legend(loc='upper left')
plt.hlines(y=0, xmin=-10, xmax=50, color='black', lw=2)
plt.xlim([-10, 50])
plt.show()

#Group3: SVR with Kfold CV
print('Group 3 SVR')
svr = svm.SVR(kernel='linear')
svr.fit(X3_train, y3_train)
cv = KFold(5, shuffle=True, random_state=33)
scores_train3 = cross_val_score(svr, X3_train, y3_train, cv=cv)
print("Average accuracy score on training set using 5-fold cross-validation:",np.mean(scores_train3))
scores_test3 = cross_val_score(svr, X3_test, y3_test, cv=cv)
print("Average accuracy score on testing set using 5-fold cross-validation:",np.mean(scores_test3))
y3_train_pred = svr.predict(X3_train)
print("Predictions of train set:")
print(y3_train_pred)
y3_test_pred = svr.predict(X3_test)
print("Predictions of test set:")
print(y3_test_pred)
rmse = np.sqrt(mean_squared_error(y3_train,y3_train_pred))
print("Root Mean Squared Error of Train Set: {}".format(rmse))
rmse = np.sqrt(mean_squared_error(y3_test,y3_test_pred))
print("Root Mean Squared Error of Test Set: {}".format(rmse))
print('Slope:')
print(svr.coef_)
print('Intercept: %.3f' % svr.intercept_)
plt.figure()
plt.scatter(y3_train_pred, y3_train_pred - y3_train, c='steelblue', marker='o', edgecolor='white', label='Training data')
plt.scatter(y3_test_pred, y3_test_pred - y3_test, c='limegreen', marker='s', edgecolor='white', label='Test data')
plt.xlabel('Predicted values')
plt.ylabel('Residuals')
plt.title('SVR Regression')
plt.legend(loc='upper left')
plt.hlines(y=0, xmin=-10, xmax=50, color='black', lw=2)
plt.xlim([-10, 50])
plt.show()

#GridCV--Using the best model above: Ridge
#Group 1
param_grid={'alpha': np.arange(1,20)}
ridge = Ridge()
ridge_cv=GridSearchCV(ridge,param_grid,scoring='r2')
ridge_cv.fit(X1_train, y1_train)
print('\nThe best alpha by GridSearchCV')
print('Optimal alpha is:{}'.format(ridge_cv.best_params_))
print('Optimal accuracy score is:{}'.format(ridge_cv.best_score_))

#Group 2
param_grid={'alpha': np.arange(1,20)}
ridge = Ridge()
ridge_cv=GridSearchCV(ridge,param_grid,scoring='r2')
ridge_cv.fit(X2_train, y2_train)
print('\nThe best alpha by GridSearchCV')
print('Optimal alpha is:{}'.format(ridge_cv.best_params_))
print('Optimal accuracy score is:{}'.format(ridge_cv.best_score_))

#Group 3GridSearchCV
param_grid={'alpha': np.arange(1,20)}
ridge = Ridge()
ridge_cv=GridSearchCV(ridge,param_grid,scoring='r2')
ridge_cv.fit(X3_train, y3_train)
print('\nThe best alpha by ')
print('Optimal alpha is:{}'.format(ridge_cv.best_params_))
print('Optimal accuracy score is:{}'.format(ridge_cv.best_score_))

#Random forest Group 1
forest = RandomForestRegressor()
forest.fit(X1_train, y1_train)
cv = KFold(5, shuffle=True, random_state=33)
print(forest.score(X1_test, y1_test))
scores_train1 = cross_val_score(forest, X1_train, y1_train, cv=cv)
print("Average accuracy score on training set using 5-fold cross-validation:",np.mean(scores_train1))
scores_test1 = cross_val_score(forest, X1_test, y1_test, cv=cv)
print("Average accuracy score on testing set using 5-fold cross-validation:",np.mean(scores_test1))
y1_train_pred = forest.predict(X1_train)
print("Predictions of train set:")
print(y1_train_pred)
y1_test_pred = forest.predict(X1_test)
print("Predictions of test set:")
print(y1_test_pred)
rmse = np.sqrt(mean_squared_error(y1_train,y1_train_pred))
print("Root Mean Squared Error of Train Set: {}".format(rmse))
rmse = np.sqrt(mean_squared_error(y1_test,y1_test_pred))
print("Root Mean Squared Error of Test Set: {}".format(rmse))
plt.figure()
plt.scatter(y1_train_pred, y1_train_pred - y1_train, c='steelblue', marker='o', edgecolor='white', label='Training data')
plt.scatter(y1_test_pred, y1_test_pred - y1_test, c='limegreen', marker='s', edgecolor='white', label='Test data')
plt.xlabel('Predicted values')
plt.ylabel('Residuals')
plt.title('Random Forest')
plt.legend(loc='upper left')
plt.hlines(y=0, xmin=-10, xmax=50, color='black', lw=2)
plt.xlim([-10, 50])
plt.show()

#Random forest Group 2
forest = RandomForestRegressor()
forest.fit(X2_train, y2_train)
cv = KFold(5, shuffle=True, random_state=33)
print(forest.score(X2_test, y2_test))
scores_train2 = cross_val_score(forest, X2_train, y2_train, cv=cv)
print("Average accuracy score on training set using 5-fold cross-validation:",np.mean(scores_train2))
scores_test2 = cross_val_score(forest, X2_test, y2_test, cv=cv)
print("Average accuracy score on testing set using 5-fold cross-validation:",np.mean(scores_test2))
y2_train_pred = forest.predict(X2_train)
print("Predictions of train set:")
print(y2_train_pred)
y2_test_pred = forest.predict(X2_test)
print("Predictions of test set:")
print(y2_test_pred)
rmse = np.sqrt(mean_squared_error(y2_train,y2_train_pred))
print("Root Mean Squared Error of Train Set: {}".format(rmse))
rmse = np.sqrt(mean_squared_error(y2_test,y2_test_pred))
print("Root Mean Squared Error of Test Set: {}".format(rmse))
plt.figure()
plt.scatter(y2_train_pred, y2_train_pred - y2_train, c='steelblue', marker='o', edgecolor='white', label='Training data')
plt.scatter(y2_test_pred, y2_test_pred - y2_test, c='limegreen', marker='s', edgecolor='white', label='Test data')
plt.xlabel('Predicted values')
plt.ylabel('Residuals')
plt.title('Random Forest')
plt.legend(loc='upper left')
plt.hlines(y=0, xmin=-10, xmax=50, color='black', lw=2)
plt.xlim([-10, 50])
plt.show()

#Random forest Group 3
forest = RandomForestRegressor()
param_grid={'n_estimators': np.arange(1,1002,50),'max_depth':np.arange(1,5)}
ridge_cv=GridSearchCV(forest,param_grid,scoring='r2')
ridge_cv.fit(X3_train, y3_train)
print('\nThe best alpha by GridSearchCV')
print('Optimal alpha is:{}'.format(ridge_cv.best_params_))
print('Optimal accuracy score is:{}'.format(ridge_cv.best_score_))

forest = RandomForestRegressor()
param_grid={'n_estimators': [151],'max_depth':[4]}
ridge_cv=GridSearchCV(forest,param_grid,scoring='r2')
ridge_cv.fit(X3_train, y3_train)
print('\nThe best alpha by GridSearchCV')
print('Optimal alpha is:{}'.format(ridge_cv.best_params_))
print('Optimal accuracy score is:{}'.format(ridge_cv.best_score_))

cv = KFold(5, shuffle=True, random_state=33)
print(forest.score(X3_test, y3_test))
scores_train3 = cross_val_score(forest, X3_train, y3_train, cv=cv)
print("Average accuracy score on training set using 5-fold cross-validation:",np.mean(scores_train3))
scores_test2 = cross_val_score(forest, X3_test, y3_test, cv=cv)
print("Average accuracy score on testing set using 5-fold cross-validation:",np.mean(scores_test3))
y3_train_pred = forest.predict(X3_train)
print("Predictions of train set:")
print(y3_train_pred)
y3_test_pred = forest.predict(X3_test)
print("Predictions of test set:")
print(y3_test_pred)
rmse = np.sqrt(mean_squared_error(y3_train,y3_train_pred))
print("Root Mean Squared Error of Train Set: {}".format(rmse))
rmse = np.sqrt(mean_squared_error(y3_test,y3_test_pred))
print("Root Mean Squared Error of Test Set: {}".format(rmse))
plt.figure()
plt.scatter(y3_train_pred, y3_train_pred - y3_train, c='steelblue', marker='o', edgecolor='white', label='Training data')
plt.scatter(y3_test_pred, y3_test_pred - y3_test, c='limegreen', marker='s', edgecolor='white', label='Test data')
plt.xlabel('Predicted values')
plt.ylabel('Residuals')
plt.title('Random Forest')
plt.legend(loc='upper left')
plt.hlines(y=0, xmin=-10, xmax=50, color='black', lw=2)
plt.xlim([-10, 50])
plt.show()